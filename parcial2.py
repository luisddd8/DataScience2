# -*- coding: utf-8 -*-
"""Parcial2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QHNTS2YIV3d-vg9H6VuWgt-sl4p3Ylqw

*SCV*
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.impute import SimpleImputer # Import SimpleImputer for handling missing values


# Cargar el dataset (esto cambiará en Colab)
data = pd.read_csv('vehicle.csv')  # Asumo que el archivo se llamará así después de descargarlo

# El dataset Statlog no tiene las columnas day/month/year/Classes como el original
# Las columnas son 18 atributos numéricos y la última es la clase (van, saab, bus, opel)
X = data.drop(columns=['class'])  # Eliminamos la columna de clase
y = data['class']  # Esta es nuestra variable objetivo

# Codificar las etiquetas de clase (van=0, saab=1, bus=2, opel=3)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Convertir todas las columnas a numéricas (por si hay algún problema)
X = X.apply(pd.to_numeric, errors='coerce')

print(data.head(7))

# Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Imputar valores faltantes con la media antes de escalar
imputer = SimpleImputer(strategy='mean')  # Crear una instancia de SimpleImputer
X_train = imputer.fit_transform(X_train)  # Ajustar y transformar los datos de entrenamiento
X_test = imputer.transform(X_test)  # Transformar los datos de prueba usando el imputador ajustado

# Escalar datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm = SVC(probability=True)  # Añadimos probability=True para poder usar predict_proba

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto'],
    'kernel': ['linear', 'rbf', 'poly'],
    'degree': [2, 3, 4]
}

grid_search = GridSearchCV(svm, param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)

best_svm = grid_search.best_estimator_
y_pred = best_svm.predict(X_test_scaled)

accuracySVM = accuracy_score(y_test, y_pred)
precisionSVM = precision_score(y_test, y_pred, average='weighted')  # Usamos weighted por ser multiclase
recallSVM = recall_score(y_test, y_pred, average='weighted')
f1SVM = f1_score(y_test, y_pred, average='weighted')

# AUC y ROC (adaptado para multiclase)
y_probs = best_svm.predict_proba(X_test_scaled)
auc = roc_auc_score(y_test, y_probs, multi_class='ovo')  # One-vs-One para multiclase

print(f"Accuracy: {accuracySVM}")
print(f"Precision: {precisionSVM}")
print(f"Recall: {recallSVM}")
print(f"F1 Score: {f1SVM}")
print(f"AUC: {auc}")

# Graficar ROC para cada clase
plt.figure(figsize=(10, 6))
for i in range(len(label_encoder.classes_)):
    fpr, tpr, _ = roc_curve(y_test, y_probs[:, i], pos_label=i)
    plt.plot(fpr, tpr, label=f'Class {label_encoder.classes_[i]} (AUC = {roc_auc_score(y_test == i, y_probs[:, i]):.2f})')

plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Multiclass')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""DecisionTreeClassifier"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer # Import SimpleImputer

# Cargar el dataset
data = pd.read_csv('vehicle.csv')  # Asumimos que el archivo se llama vehicle.csv

# Ver las primeras filas para entender la estructura
print(data.head())

# Preprocesamiento
data.columns = data.columns.str.strip()  # Limpiar nombres de columnas
X = data.drop(columns=['class'])  # Eliminar la columna de clase
y = data['class']  # Variable objetivo

# Codificar las etiquetas de clase
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Convertir todas las columnas a numéricas
X = X.apply(pd.to_numeric, errors='coerce')

# Dividir en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Imputar valores faltantes con la media ANTES de aplicar PCA
imputer = SimpleImputer(strategy='mean')  # Crear una instancia de SimpleImputer
X_train = imputer.fit_transform(X_train)  # Ajustar y transformar los datos de entrenamiento
X_test = imputer.transform(X_test)  # Transformar los datos de prueba usando el imputador ajustado

# Aplicar PCA
pca = PCA(n_components=X_train.shape[1])
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
# Aplicar PCA
pca = PCA(n_components=X_train.shape[1])
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Crear y entrenar el modelo de árbol de decisión
dt_classifier = DecisionTreeClassifier()

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_classifier, param_grid, cv=5)
grid_search.fit(X_train_pca, y_train)

best_dt = grid_search.best_estimator_
y_pred = best_dt.predict(X_test_pca)

# Calcular métricas (usando average='weighted' para problemas multiclase)
accuracyDT = accuracy_score(y_test, y_pred)
precisionDT = precision_score(y_test, y_pred, average='weighted')
recallDT = recall_score(y_test, y_pred, average='weighted')
f1DT = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracyDT}")
print(f"Precision: {precisionDT}")
print(f"Recall: {recallDT}")
print(f"F1 Score: {f1DT}")

# Calcular curva ROC para cada clase (adaptación multiclase)
plt.figure(figsize=(10, 8))

# Obtener las probabilidades para cada clase
y_score = best_dt.predict_proba(X_test_pca)

# Calcular ROC para cada clase
n_classes = len(label_encoder.classes_)
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test == i, y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='No Skill')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve Decision Tree - Multiclass')
plt.legend()
plt.show()

"""*RandomForestClassifier*"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.multiclass import OneVsRestClassifier

# 1. Cargar el dataset
# Primero necesitas subir el archivo vehicle.csv a Colab (instrucciones abajo)
data = pd.read_csv('vehicle.csv')

# 2. Preprocesamiento
data.columns = data.columns.str.strip()  # Limpiar nombres de columnas
X = data.drop(columns=['class'])  # Eliminar columna de clase
y = data['class']  # Variable objetivo (van, saab, bus, opel)

# Codificar las etiquetas (van=0, saab=1, bus=2, opel=3)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Convertir todas las columnas a numéricas
X = X.apply(pd.to_numeric, errors='coerce')

# 3. Dividir los datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Crear y entrenar el modelo Random Forest
# Usamos OneVsRestClassifier para manejar múltiples clases en las métricas
rf_classifier = OneVsRestClassifier(RandomForestClassifier(random_state=42))

param_grid = {
    'estimator__n_estimators': [100, 200],
    'estimator__max_depth': [None, 10, 20],
    'estimator__min_samples_split': [2, 5],
    'estimator__min_samples_leaf': [1, 2],
    'estimator__bootstrap': [True, False]
}

grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_rf_classifier = grid_search.best_estimator_
y_pred = best_rf_classifier.predict(X_test)

# 5. Métricas (usando average='weighted' para multiclase)
accuracyRF = accuracy_score(y_test, y_pred)
precisionRF = precision_score(y_test, y_pred, average='weighted')
recallRF = recall_score(y_test, y_pred, average='weighted')
f1RF = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracyRF}")
print(f"Precision: {precisionRF}")
print(f"Recall: {recallRF}")
print(f"F1 Score: {f1RF}")

# 6. Curva ROC para cada clase
plt.figure(figsize=(10, 8))

# Obtener probabilidades para cada clase
y_score = best_rf_classifier.predict_proba(X_test)

# Calcular ROC para cada clase
n_classes = len(label_encoder.classes_)
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test == i, y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='No Skill')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve RandomForest - Multiclass')
plt.legend()
plt.show()

"""*ExtraTreesClassifier*"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder, label_binarize # Import label_binarize
from sklearn.multiclass import OneVsRestClassifier
import numpy as np
from sklearn.impute import SimpleImputer # Import SimpleImputer

# 1. Cargar el dataset
# Primero necesitas subir vehicle.csv a Colab (instrucciones abajo)
data = pd.read_csv('vehicle.csv')

# 2. Preprocesamiento
data.columns = data.columns.str.strip()  # Limpiar nombres de columnas
X = data.drop(columns=['class'])  # Eliminar columna de clase
y = data['class']  # Variable objetivo (van, saab, bus, opel)

# Codificar las etiquetas (van=0, saab=1, bus=2, opel=3)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Convertir todas las columnas a numéricas
X = X.apply(pd.to_numeric, errors='coerce')

# 3. Dividir los datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ----> Apply SimpleImputer BEFORE PCA
# Imputar valores faltantes con la media
imputer = SimpleImputer(strategy='mean')  # Crear una instancia de SimpleImputer
X_train = imputer.fit_transform(X_train)  # Ajustar y transformar los datos de entrenamiento
X_test = imputer.transform(X_test)  # Transformar los datos de prueba usando el imputador ajustado

# 4. Aplicar PCA
pca = PCA(n_components=0.95)  # Mantener el 95% de varianza
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

print(f"Reduced from {X_train.shape[1]} to {X_train_pca.shape[1]} dimensions")

# 5. Crear y entrenar el modelo Extra Trees
# Usamos OneVsRest para manejar múltiples clases
et_classifier = OneVsRestClassifier(ExtraTreesClassifier(random_state=42))

param_grid = {
    'estimator__n_estimators': [100, 200],
    'estimator__max_depth': [None, 10, 20],
    'estimator__min_samples_split': [2, 5],
    'estimator__min_samples_leaf': [1, 2],
    'estimator__bootstrap': [True, False]
}

grid_search = GridSearchCV(et_classifier, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train_pca, y_train)

best_et_classifier = grid_search.best_estimator_
y_pred = best_et_classifier.predict(X_test_pca)

# 6. Métricas (usando average='weighted' para multiclase)
accuracyETC = accuracy_score(y_test, y_pred)
precisionETC = precision_score(y_test, y_pred, average='weighted')
recallETC = recall_score(y_test, y_pred, average='weighted')
f1ETC = f1_score(y_test, y_pred, average='weighted')

print("\nBest Parameters:", grid_search.best_params_)
print(f"\nAccuracy: {accuracyETC:.4f}")
print(f"Precision: {precisionETC:.4f}")
print(f"Recall: {recallETC:.4f}")
print(f"F1 Score: {f1ETC:.4f}")

# 7. Curva ROC multiclase
plt.figure(figsize=(10, 8))

# Obtener probabilidades para cada clase
y_score = best_et_classifier.predict_proba(X_test_pca)

# Binarizar las etiquetas para multiclase
y_test_bin = label_binarize(y_test, classes=np.unique(y))

# Calcular ROC para cada clase
for i in range(len(label_encoder.classes_)):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='No Skill')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Extra Trees (Multiclass)')
plt.legend(loc='lower right')
plt.grid()
plt.show()